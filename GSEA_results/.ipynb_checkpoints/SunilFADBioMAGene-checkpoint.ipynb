{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6e4f2a99",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import collections\n",
    "import random\n",
    "\n",
    "import numpy as np\n",
    "from numpy import loadtxt\n",
    "import pandas as pd\n",
    "import scipy\n",
    "\n",
    "import gseapy as gp #paper for GSEApy: https://academic.oup.com/bioinformatics/article/39/1/btac757/6847088\n",
    "from gseapy import gseaplot\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4c4202a",
   "metadata": {},
   "source": [
    "### Variables requiring user input - change as needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5e3ddd06",
   "metadata": {},
   "outputs": [],
   "source": [
    "gene_query_set_size = 500 #500 is recommended for optimal coverage without introducing statistical bias\n",
    "GSEAresultsfiletitle = '\\gseapy.gene_set.prerank.report.csv' #this changes if GSEApy is updated\n",
    "\n",
    "#probe level analysis variables\n",
    "zscorebool = True #set to True if you want to run z-score\n",
    "Tscorebool = True #set to True if you want to run T-score\n",
    "GSEAgeneIDbool = True #set to True if you want to run GSEA to identify probes of interest\n",
    "genepanelbool = True #set to True if you want to define panels\n",
    "moregenedatasetsbool = True #set to True if you have additional datasets for meta-analysis\n",
    "randomgenemodel = True #set to True if you want ran random model for additional datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f048842e",
   "metadata": {},
   "source": [
    "### Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d6cff015",
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_prep(input_file_dir, output_path):\n",
    "    global zscorebool, dataset_list, identification_datasets\n",
    "\n",
    "    \n",
    "    input_path_list = []    \n",
    "\n",
    "    #get list of subdirectories - should only have \"input\" folder in the directory\n",
    "    for path in glob.glob(f'{cwd}/*/**/', recursive=True):\n",
    "        input_path_list.append(path)\n",
    "\n",
    "    for input_path in input_path_list: #loop for more than just 1 input file (BioMOMA)\n",
    "        path_split_list = input_path.split(\"\\\\\")\n",
    "        input_file_name = path_split_list[-2]    \n",
    "        input_file_name = input_file_name[:len(input_file_name) - 5]\n",
    "\n",
    "        #establish path variable for output directory\n",
    "        all_files = []\n",
    "        dataset_list = []\n",
    "        samples_data_list = []\n",
    "\n",
    "        #iterate input directory to generate a list of all files (all_files)\n",
    "        for path in os.listdir(input_path):\n",
    "            if os.path.isfile(os.path.join(input_path, path)): #check if current path is a file\n",
    "                all_files.append(path)\n",
    "\n",
    "        #check to see files are properly paired\n",
    "        compare_dataset_IDs = []\n",
    "        for file_name in all_files:\n",
    "            if \"samples\" in file_name:\n",
    "                file_name = file_name[:-11:]\n",
    "                samples_data_list.append(file_name)\n",
    "            elif \"data\" in file_name:\n",
    "                file_name = file_name[:-8:]\n",
    "                dataset_list.append(file_name)\n",
    "\n",
    "        #sorting both the lists so order does not impede check\n",
    "        samples_data_list.sort()\n",
    "        dataset_list.sort()\n",
    "        if samples_data_list != dataset_list: #using == to check if lists are equal\n",
    "            print('Missing dataset or samples data file.')\n",
    "\n",
    "\n",
    "        #PANEL IDENTIFICATION -------------------------------------------------------------------------------------------------\n",
    "        number_ID_datasets = 0 #number of datasets that store identification data (must end up being 1 or 2)\n",
    "        identification_datasets = [] #list of identification datasets\n",
    "\n",
    "\n",
    "        for dataset in dataset_list:\n",
    "            if dataset[-2:] == \"ID\": #determine if file_name is identification dataset\n",
    "                if number_ID_datasets == 0:\n",
    "                    probe_ID_file1 = \"\\\\\" + dataset + \"data.csv\" #name of first file that stores identification data\n",
    "                    sample_ID_file1 = \"\\\\\" + dataset + \"samples.txt\" #name of file that stores sample identification data for first file\n",
    "                    output_path1 = output_path + '\\GSEAID' + dataset #establish path variable for identification output directory\n",
    "                    output_path2 = output_path + '\\GSEAID2' + dataset #establish path variable for identification output directory\n",
    "                    print('The identification dataset is', dataset[:-2])\n",
    "                    identification_datasets.append(dataset)\n",
    "                if number_ID_datasets == 1:\n",
    "                    probe_ID_file2 = \"\\\\\" + dataset + \"data.csv\" #name of second file, if needed, that stores identification data\n",
    "                    sample_ID_file2 = \"\\\\\" + dataset + \"samples.txt\" #name of file that stores sample identification data for second file        \n",
    "                    output_path2 = output_path + '\\GSEAID' + dataset #rename path variable for identification output directory\n",
    "                    print('The second identification dataset is', dataset[:-2])\n",
    "                    identification_datasets.append(dataset)\n",
    "\n",
    "                number_ID_datasets = number_ID_datasets + 1\n",
    "        print()\n",
    "\n",
    "        if zscorebool == True:\n",
    "            #create output directories\n",
    "            if not os.path.exists(output_gene_file_dir):\n",
    "                os.mkdir(output_gene_file_dir)\n",
    "            if not os.path.exists(output_path1):\n",
    "                os.mkdir(output_path1)\n",
    "            if not os.path.exists(output_path2):\n",
    "                os.mkdir(output_path2)    \n",
    "\n",
    "                \n",
    "                \n",
    "        #load first identification dataset as dataframe and format it for use    \n",
    "        probe_ID_path1 = input_path + probe_ID_file1   \n",
    "        raw_df1 = pd.read_csv(probe_ID_path1, low_memory = False, index_col = None, header = None, sep = ',')\n",
    "        raw_df1.set_index(raw_df1.columns[0], inplace=True)\n",
    "\n",
    "        raw_df1_sample_list = raw_df1.iloc[0].tolist()\n",
    "        raw_df1.columns = raw_df1.iloc[0]\n",
    "        raw_df1.drop(raw_df1.index[0], inplace = True)\n",
    "\n",
    "        #load sample information for first identification dataset as a list and format it for use\n",
    "        sample_ID_path1 = input_path + sample_ID_file1\n",
    "        sample_file1 = open(sample_ID_path1, \"r\")\n",
    "        samples1 = sample_file1.read()\n",
    "        sample_list1 = samples1.split(\"\\t\")\n",
    "        sample_list1[-1] = sample_list1[-1].strip()\n",
    "\n",
    "        if len(sample_list1) == raw_df1.shape[1]:\n",
    "            print('Total number of samples confirmed for the first identification dataset.')\n",
    "            print()\n",
    "\n",
    "        if number_ID_datasets == 2:\n",
    "            #load second identification dataset as dataframe and format it for use\n",
    "            probe_ID_path2 = input_path + probe_ID_file2\n",
    "            raw_df2 = pd.read_csv(probe_ID_path2, low_memory = False, index_col = None, header = 0, sep = ',')\n",
    "            raw_df2.set_index(raw_df2.columns[0], inplace=True)\n",
    "\n",
    "            #load sample information for second identification dataset as a list and format it for use\n",
    "            sample_ID_path2 = input_path + sample_ID_file2    \n",
    "            sample_file2 = open(sample_ID_path2, \"r\")\n",
    "            samples2 = sample_file2.read()\n",
    "            sample_list2 = samples2.split(\"\\t\")\n",
    "            sample_list2[-1] = sample_list2[-1].strip()\n",
    "\n",
    "            if len(sample_list2) == raw_df2.shape[1]:\n",
    "                print('Total number of samples confirmed for the second identification dataset.')\n",
    "                print()\n",
    "                \n",
    "                \n",
    "        #define df1 and df2\n",
    "        df1 = pd.DataFrame()\n",
    "        df2 = pd.DataFrame()\n",
    "\n",
    "        #define empty variables\n",
    "        control_cols1 = []\n",
    "        control_cols2 = []\n",
    "        experimental_cols1 = []\n",
    "        experimental_cols2 = []\n",
    "\n",
    "        num = 0 #sample counter\n",
    "        if number_ID_datasets == 1:    \n",
    "            #determine column index (location) of control and experimental samples for first dataset\n",
    "            total_controls = []\n",
    "            sample_control_list = []\n",
    "            total_experimental = []\n",
    "            sample_experimental_list = []\n",
    "\n",
    "            #these variables are lists of sample indecies from df\n",
    "            temp_control_cols1 = []\n",
    "            temp_control_cols2 = []\n",
    "            temp_experimental_cols1 = []\n",
    "            temp_experimental_cols2 = []\n",
    "\n",
    "            for i in sample_list1:\n",
    "                if i == '0':\n",
    "                    total_controls.append(num)\n",
    "                    sample_control_list.append(raw_df1_sample_list[num])\n",
    "                else:\n",
    "                    total_experimental.append(num)\n",
    "                    sample_experimental_list.append(raw_df1_sample_list[num])\n",
    "                num = num + 1\n",
    "\n",
    "            #assign sample identities    \n",
    "            control_midpt = int(len(total_controls) / 2)\n",
    "            temp_control_cols1 = total_controls[:control_midpt]\n",
    "            temp_control_cols2 = total_controls[control_midpt:]\n",
    "\n",
    "            experimental_midpt = int(len(total_experimental) / 2)\n",
    "            temp_experimental_cols1 = total_experimental[:experimental_midpt]    \n",
    "            temp_experimental_cols2 = total_experimental[experimental_midpt:]    \n",
    "\n",
    "            #assign dataframes\n",
    "            name_num = 0\n",
    "            num = 0\n",
    "            for i in temp_experimental_cols1:\n",
    "                temp_col = raw_df1.iloc[:, i]       \n",
    "                sample_ID = sample_experimental_list[name_num]\n",
    "                df1[sample_ID] = temp_col\n",
    "                experimental_cols1.append(num)\n",
    "                name_num = name_num + 1\n",
    "                num = num + 1\n",
    "\n",
    "            name_num = 0\n",
    "            for i in temp_control_cols1:\n",
    "                temp_col = raw_df1.iloc[:, i]\n",
    "                sample_ID = sample_control_list[name_num]\n",
    "                df1[sample_ID] = temp_col\n",
    "                control_cols1.append(num)\n",
    "                name_num = name_num + 1\n",
    "                num = num + 1\n",
    "\n",
    "            name_num = len(experimental_cols1)\n",
    "            num = 0\n",
    "            for i in temp_experimental_cols2:\n",
    "                temp_col = raw_df1.iloc[:, i]\n",
    "                sample_ID = sample_experimental_list[name_num]\n",
    "                df2[sample_ID] = temp_col\n",
    "                experimental_cols2.append(num)\n",
    "                name_num = name_num + 1\n",
    "                num = num + 1\n",
    "\n",
    "            name_num = len(control_cols1)\n",
    "            for i in temp_control_cols2:\n",
    "                temp_col = raw_df1.iloc[:, i]\n",
    "                sample_ID = sample_control_list[name_num]\n",
    "                df2[sample_ID] = temp_col\n",
    "                control_cols2.append(num)\n",
    "                name_num = name_num + 1\n",
    "                num = num + 1\n",
    "\n",
    "        else:    \n",
    "            #assign sample identities\n",
    "            for i in sample_list1:\n",
    "                if i == '0':\n",
    "                    control_cols1.append(num)\n",
    "                else:\n",
    "                    experimental_cols1.append(num)\n",
    "                num = num + 1\n",
    "\n",
    "            num = 0\n",
    "            for i in sample_list2:\n",
    "                if i == '0':\n",
    "                    control_cols2.append(num)\n",
    "                else:\n",
    "                    experimental_cols2.append(num)\n",
    "                num = num + 1\n",
    "\n",
    "                #assign dataframes\n",
    "                df1 = raw_df1\n",
    "                df2 = raw_df2\n",
    "\n",
    "        #check this output to ensure proper column selection\n",
    "        print('Number of control samples for identification signature 1:', len(control_cols1))\n",
    "        for l in control_cols1:\n",
    "            print(df1.columns[l])\n",
    "        print()\n",
    "        print('Number of experimental samples for identification signature 1:', len(experimental_cols1))\n",
    "        for l in experimental_cols1:\n",
    "            print(df1.columns[l])\n",
    "        print()\n",
    "\n",
    "        print('Number of control samples for identification signature 2:', len(control_cols2))\n",
    "        for l in control_cols2:\n",
    "            print(df2.columns[l]) \n",
    "        print()\n",
    "        print('Number of experimental samples for identification signature 2:', len(experimental_cols2))\n",
    "        for l in experimental_cols2:\n",
    "            print(df2.columns[l])\n",
    "        print()\n",
    "    return df1, df2, output_path1, output_path2, control_cols1, experimental_cols1, control_cols2, experimental_cols2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d568ad0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#function zscore normalizes data across all samples (columns) per gene (row) \n",
    "def zscore(df, datasetnum, loc):\n",
    "    df.dropna(inplace = True)\n",
    "    df.reset_index(inplace=True)\n",
    "    totalrows = df[df.columns[0]].count()\n",
    "    df2 = pd.DataFrame()\n",
    "    tempnum = 0 #row counter variable\n",
    "    \n",
    "    while tempnum < totalrows:\n",
    "        zscorearray = []\n",
    "        temparray = df.iloc[tempnum].to_numpy()\n",
    "        geneID = temparray[0]\n",
    "        temparray = np.delete(temparray, 0, 0)\n",
    "        temparray = temparray.astype(np.float64)\n",
    "\n",
    "        mean = np.mean(temparray)\n",
    "        std = np.std(temparray)\n",
    "\n",
    "        for value in temparray:\n",
    "            tempz = (value - mean) / std\n",
    "            zscorearray = np.append(zscorearray, tempz)\n",
    "        tempseries = pd.Series(zscorearray, name = geneID)\n",
    "        df2 = pd.concat([df2, tempseries.to_frame().T])\n",
    "        \n",
    "        if tempnum % 1000 == 0:\n",
    "            print('Z-scoring gene #' + str(tempnum))\n",
    "        tempnum = tempnum + 1\n",
    "        \n",
    "    df2copy = df2\n",
    "    header = list(df.head(0))\n",
    "    header.pop(0)\n",
    "    df2.columns = header\n",
    "    filename = loc + '\\Zscoredata.txt'\n",
    "    df2.to_csv(filename, sep='\\t', index=True)\n",
    "    print('Z-score done for dataset ' + str(datasetnum))\n",
    "    print()\n",
    "    return df2copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d07c06d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#function Tscores each gene between defined control and experimental groups\n",
    "def Tscore(df, controlcols, experimentalcols, datasetnum, loc):\n",
    "    dfstat = pd.DataFrame()\n",
    "    dfpval = pd.DataFrame()\n",
    "    tempnum = 0 #row counter variable\n",
    "    \n",
    "    for r in df.iterrows():\n",
    "        geneID = r[0]\n",
    "        row = r[1]\n",
    "        controlvals = []\n",
    "        experimentalvals = []\n",
    "        for iter in controlcols:\n",
    "            controlvals.append(row[iter])\n",
    "        for iter in experimentalcols:\n",
    "            experimentalvals.append(row[iter])\n",
    "    \n",
    "        stat, pval = scipy.stats.ttest_ind(experimentalvals, controlvals, equal_var=False)\n",
    "        tempstatseries = pd.Series(stat, name = geneID)\n",
    "        temppvalseries = pd.Series(pval, name = geneID)\n",
    "        \n",
    "        dfstat = pd.concat([dfstat, tempstatseries.to_frame().T])\n",
    "        dfpval = pd.concat([dfpval, temppvalseries.to_frame().T])\n",
    "        \n",
    "        if tempnum % 1000 == 0:\n",
    "            print('T-scoring gene #' + str(tempnum))\n",
    "        tempnum = tempnum + 1\n",
    "        \n",
    "    dfstat.columns = ['Tscore']\n",
    "    df2 = pd.concat([dfstat, dfpval], axis = 1)\n",
    "    df2.columns = ['Tscore', 'pval']\n",
    "    filename = loc + '\\Tscoredata.txt'\n",
    "    df2.to_csv(filename, sep='\\t', index=True)\n",
    "    dfstat = dfstat.sort_values(by=['Tscore'], ascending=False)\n",
    "    print('T-score done for dataset ' + str(datasetnum))\n",
    "    print()\n",
    "    return dfstat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5fe8fbe0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#function removes duplicate genes by finding the highest abs(Tscore) \n",
    "#function then remove genes without Tscores and adjusts centers signature\n",
    "def dups(signature, datasetnum, loc):\n",
    "    sigdict = {}\n",
    "    num = 0\n",
    "\n",
    "    for gene, row in signature.iterrows():\n",
    "        tempTscore = signature.iloc[num]['Tscore']\n",
    "        if np.isnan(tempTscore) == False:\n",
    "            if gene in sigdict:   \n",
    "                if abs(tempTscore) > abs(row['Tscore']):\n",
    "                    sigdict.update({gene:tempTscore})\n",
    "            else:\n",
    "                sigdict.update({gene:row['Tscore']})\n",
    "        else:\n",
    "            print(gene, tempTscore)\n",
    "        num = num + 1\n",
    "\n",
    "    shortsig = pd.DataFrame(list(sigdict.items()),columns = ['Gene','Tscore']) \n",
    "    \n",
    "    #center signature\n",
    "    midptloc = int((len(shortsig.index)) / 2)\n",
    "    midTscore = shortsig.iloc[midptloc] ['Tscore']\n",
    "    newTscore = []\n",
    "    \n",
    "    for i, row in shortsig.iterrows():\n",
    "        newval = row['Tscore'] - midTscore\n",
    "        newTscore.append(float(newval))\n",
    "    shortsig['Tscoreadjusted'] = newTscore\n",
    "    shortsig.set_index(\"Gene\", inplace = True)\n",
    "    print('Tscores for dataset', str(datasetnum), 'were adjusted by', (-1*midTscore))\n",
    "    print()\n",
    "    \n",
    "    filename = loc + '\\signature.txt'\n",
    "    shortsig.to_csv(filename, sep='\\t', index=True)\n",
    "    shortsig.drop('Tscore', axis=1, inplace = True)\n",
    "    shortsig.rename(columns={\"Tscoreadjusted\": \"Tscore\"}, inplace=True)\n",
    "    return shortsig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "47b69433",
   "metadata": {},
   "outputs": [],
   "source": [
    "#function generates query sets from tails of 2 signatures established in prior functions\n",
    "def query_generator(df, querysetsize, datasetnum, loc, query_num):\n",
    "    global GSEAgeneIDbool, GSEApathIDbool\n",
    "    \n",
    "    dfquery = pd.DataFrame()\n",
    "    if query_num == 1:\n",
    "        dfhighest = df['Tscore'].nlargest(querysetsize)\n",
    "        dflowest = df['Tscore'].nsmallest(querysetsize)\n",
    "    if query_num == 2:\n",
    "        dfhighest = df['NES'].nlargest(querysetsize)\n",
    "        dflowest = df['NES'].nsmallest(querysetsize)\n",
    "\n",
    "    highestlist = pd.Index.tolist(dfhighest.index)\n",
    "    lowestlist = pd.Index.tolist(dflowest.index)\n",
    "    \n",
    "    highestlist.insert(0, ('positivetail' + str(datasetnum)))\n",
    "    highestlist.insert(1, ('spacer'))\n",
    "    \n",
    "    lowestlist.insert(0, ('negativetail' + str(datasetnum)))\n",
    "    lowestlist.insert(1, ('spacer'))\n",
    "    \n",
    "    highestSeries = pd.Series(highestlist)\n",
    "    lowestSeries = pd.Series(lowestlist)\n",
    "    dfquery = pd.concat([highestSeries, lowestSeries], axis = 1)\n",
    "    dfquery = dfquery.T\n",
    "    print('Query set', str(datasetnum), 'generation done!')\n",
    "    print()\n",
    "    return dfquery"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9ae41a2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#function performs GSEA\n",
    "def prerank_GSEA(refsigdf, gene_sets, outputdir, size):\n",
    "    #create GSEA results output directory\n",
    "    new_output_dir = outputdir + '\\GSEAresults'\n",
    "    if not os.path.exists(new_output_dir):\n",
    "        os.mkdir(new_output_dir)\n",
    "    \n",
    "    #run GSEA\n",
    "    preresult = gp.prerank(rnk=refsigdf,\n",
    "                     gene_sets=gene_sets,\n",
    "                     min_size=15,\n",
    "                     max_size=size,\n",
    "                     permutation_num=1000,\n",
    "                     outdir=new_output_dir,\n",
    "                     seed=6,\n",
    "                     verbose=False, \n",
    "                    )\n",
    "    \n",
    "    #print GSEA results to screen\n",
    "    print(preresult.res2d.head(5))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e764603",
   "metadata": {},
   "source": [
    "### Main program"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c2a184cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "cwd = os.getcwd() #gets current location of program file (.jpynb)\n",
    "\n",
    "output_gene_file_dir = cwd + \"\\\\\" + \"geneleveloutput\" #name of directory to store output data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7f028ddc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "275e0def",
   "metadata": {},
   "source": [
    "#### Z-score code - this section of blocks determine if zscores need to be calculated and imports them if available"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfc25aa5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The identification dataset is GSE167000v1\n",
      "The second identification dataset is GSE167000v2\n",
      "\n",
      "Total number of samples confirmed for the first identification dataset.\n",
      "\n",
      "Total number of samples confirmed for the second identification dataset.\n",
      "\n",
      "Number of control samples for identification signature 1: 10\n",
      "CUcovID_0006\n",
      "CUcovID_0017\n",
      "CUcovID_0023\n",
      "CUcovID_0027\n",
      "CUcovID_0028\n",
      "CUcovID_0031\n",
      "CUcovID_0037\n",
      "CUcovID_0038\n",
      "CUcovID_0039\n",
      "CUcovID_0044\n",
      "\n",
      "Number of experimental samples for identification signature 1: 22\n",
      "CUcovID_0001\n",
      "CUcovID_0002\n",
      "CUcovID_0003\n",
      "CUcovID_0005\n",
      "CUcovID_0007\n",
      "CUcovID_0008\n",
      "CUcovID_0009\n",
      "CUcovID_0010\n",
      "CUcovID_0011\n",
      "CUcovID_0012\n",
      "CUcovID_0013\n",
      "CUcovID_0014\n",
      "CUcovID_0016\n",
      "CUcovID_0018\n",
      "CUcovID_0019\n",
      "CUcovID_0020\n",
      "CUcovID_0021\n",
      "CUcovID_0022\n",
      "CUcovID_0024\n",
      "CUcovID_0025\n",
      "CUcovID_0026\n",
      "CUcovID_0029\n",
      "\n",
      "Number of control samples for identification signature 2: 10\n",
      "CUcovID_0048\n",
      "CUcovID_0051\n",
      "CUcovID_0052\n",
      "CUcovID_0063\n",
      "CUcovID_0064\n",
      "CUcovID_0072\n",
      "CUcovID_0073\n",
      "CUcovID_0076\n",
      "CUcovID_0079\n",
      "CUcovID_0081\n",
      "\n",
      "Number of experimental samples for identification signature 2: 22\n",
      "CUcovID_0030\n",
      "CUcovID_0033\n",
      "CUcovID_0035\n",
      "CUcovID_0036\n",
      "CUcovID_0040\n",
      "CUcovID_0041\n",
      "CUcovID_0042\n",
      "CUcovID_0043\n",
      "CUcovID_0045\n",
      "CUcovID_0046\n",
      "CUcovID_0047\n",
      "CUcovID_0049\n",
      "CUcovID_0050\n",
      "CUcovID_0054\n",
      "CUcovID_0055\n",
      "CUcovID_0056\n",
      "CUcovID_0057\n",
      "CUcovID_0059\n",
      "CUcovID_0060\n",
      "CUcovID_0061\n",
      "CUcovID_0065\n",
      "CUcovID_0066\n",
      "\n",
      "Z-scoring gene #0\n",
      "Z-scoring gene #1000\n",
      "Z-scoring gene #2000\n",
      "Z-scoring gene #3000\n",
      "Z-scoring gene #4000\n",
      "Z-scoring gene #5000\n",
      "Z-scoring gene #6000\n",
      "Z-scoring gene #7000\n",
      "Z-scoring gene #8000\n",
      "Z-scoring gene #9000\n",
      "Z-scoring gene #10000\n",
      "Z-scoring gene #11000\n",
      "Z-scoring gene #12000\n",
      "Z-scoring gene #13000\n",
      "Z-scoring gene #14000\n",
      "Z-scoring gene #15000\n",
      "Z-scoring gene #16000\n",
      "Z-scoring gene #17000\n",
      "Z-scoring gene #18000\n",
      "Z-scoring gene #19000\n",
      "Z-scoring gene #20000\n",
      "Z-scoring gene #21000\n",
      "Z-scoring gene #22000\n",
      "Z-scoring gene #23000\n",
      "Z-scoring gene #24000\n",
      "Z-scoring gene #25000\n",
      "Z-scoring gene #26000\n",
      "Z-scoring gene #27000\n",
      "Z-scoring gene #28000\n",
      "Z-scoring gene #29000\n",
      "Z-scoring gene #30000\n",
      "Z-scoring gene #31000\n",
      "Z-scoring gene #32000\n",
      "Z-scoring gene #33000\n",
      "Z-scoring gene #34000\n",
      "Z-scoring gene #35000\n",
      "Z-scoring gene #36000\n",
      "Z-scoring gene #37000\n"
     ]
    }
   ],
   "source": [
    "if zscorebool == True: #split data into 4 groups and normalize data \n",
    "    input_file_dir = cwd + \"\\\\\" + \"input\" #name of directory to store input data\n",
    "    df1, df2, output_path1, output_path2, control_cols1, experimental_cols1, control_cols2, experimental_cols2 = data_prep(input_file_dir, output_gene_file_dir)\n",
    "    \n",
    "    #call function to zscore data\n",
    "    normalized_df1 = zscore(df1, 1, output_path1)\n",
    "    normalized_df2 = zscore(df2, 2, output_path2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0216074f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#establish a list of output folders\n",
    "output_path_list = [] #list of output dictionary folders\n",
    "\n",
    "for path in glob.glob(f'{output_gene_file_dir}/*/', recursive=True): #get list of subdirectories for \\\\geneleveloutput directory\n",
    "    output_path_list.append(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24c55c71",
   "metadata": {},
   "outputs": [],
   "source": [
    "if zscorebool == False:\n",
    "    if Tscorebool == True: #bring in zscore files if needed for T-score function\n",
    "        input_file_dir = cwd + \"\\\\\" + \"input\" #name of directory to store input data        \n",
    "        count = 1 #local folder counter\n",
    "        for i in output_path_list:\n",
    "            temp_output_file_dir = i + \"\\\\\" + \"Zscoredata\" + str(count) + \".txt\" #name of directory to store output data\n",
    "            if count == 1:\n",
    "                normalized_df1 = pd.read_csv(temp_output_file_dir, low_memory = False, index_col = None, header = 0, sep = '\\t')\n",
    "                normalized_df1.set_index(normalized_df1.columns[0], inplace=True)\n",
    "                df1, df2, output_path1, output_path2, control_cols1, experimental_cols1, control_cols2, experimental_cols2 = data_prep(input_file_dir, output_file_dir)\n",
    "\n",
    "                count = count + 1\n",
    "            else:\n",
    "                normalized_df2 = pd.read_csv(temp_output_file_dir, low_memory = False, index_col = None, header = 0, sep = '\\t')\n",
    "                normalized_df2.set_index(normalized_df2.columns[0], inplace=True)  \n",
    "                df1, df2, output_path1, output_path2, control_cols1, experimental_cols1, control_cols2, experimental_cols2 = data_prep(input_file_dir, output_file_dir)\n",
    "        \n",
    "        print(normalized_df1)\n",
    "       \n",
    "    else: #Tscorebool = False\n",
    "        print('This section has been skipped.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad3cd675",
   "metadata": {},
   "source": [
    "#### T-score code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c981029",
   "metadata": {},
   "outputs": [],
   "source": [
    "if Tscorebool == True: #run Tscore function from functions script\n",
    "    temp_ref_sig_df1 = Tscore(normalized_df1, control_cols1, experimental_cols1, 1, output_path1)\n",
    "    temp_ref_sig_df2 = Tscore(normalized_df2, control_cols2, experimental_cols2, 2, output_path2)\n",
    "    \n",
    "    #calls function to remove duplicates and adjust T-scored signatures\n",
    "    ref_sig_df1 = dups(temp_ref_sig_df1, 1, output_path1)\n",
    "    ref_sig_df2 = dups(temp_ref_sig_df2, 2, output_path2)\n",
    "      \n",
    "else: #Tscorebool = False\n",
    "    if GSEAgeneIDbool == True: #bring in Tscore files IF NEEDED\n",
    "        count = 1 #local folder counter\n",
    "        for i in output_path_list:\n",
    "            temp_output_file_dir = i + \"\\\\\" + \"Tscoredata\" + str(count) + \".txt\" #name of directory to store output data\n",
    "            if count == 1:\n",
    "                temp_ref_sig_df1 = pd.read_csv(temp_output_file_dir, low_memory = False, index_col = None, header = 0, sep = '\\t')\n",
    "                temp_ref_sig_df1.set_index(temp_ref_sig_df1.columns[0], inplace=True)\n",
    "                count = count + 1\n",
    "            else:\n",
    "                temp_ref_sig_df2 = pd.read_csv(temp_output_file_dir, low_memory = False, index_col = None, header = 0, sep = '\\t')\n",
    "                temp_ref_sig_df2.set_index(temp_ref_sig_df2.columns[0], inplace=True)   \n",
    "        \n",
    "        print(temp_ref_sig_df1)\n",
    "    else:\n",
    "        print('This section has been skipped.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc861704",
   "metadata": {},
   "source": [
    "#### GSEA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7eb8a076",
   "metadata": {},
   "outputs": [],
   "source": [
    "if GSEAgeneIDbool == True: #run GSEA function from functions script \n",
    "    #calls function to define query sets (4 total) from tails of the 2 signatures\n",
    "    query_df1 = query_generator(ref_sig_df1, gene_query_set_size, 1, output_path1, 1) \n",
    "    query_df2 = query_generator(ref_sig_df2, gene_query_set_size, 2, output_path2, 1)\n",
    "    query_df = pd.concat([query_df1, query_df2], axis = 0)\n",
    "    tempinputloc = output_gene_file_dir + '\\querygenesetdata.gmt'\n",
    "    query_df.to_csv(tempinputloc, sep='\\t', index=False, header = None)\n",
    "\n",
    "    #calls function to perform GSEA\n",
    "    inputdir = output_gene_file_dir + '\\querygenesetdata.gmt'    \n",
    "    \n",
    "    prerank_GSEA(ref_sig_df1, inputdir, output_path1, gene_query_set_size) #need to fix to import GSEA function\n",
    "    prerank_GSEA(ref_sig_df2, inputdir, output_path2, gene_query_set_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbeb6ed8",
   "metadata": {},
   "outputs": [],
   "source": [
    "if genepanelbool == True: #bring in GSEA results files if needed to define gene panels\n",
    "    count = 1\n",
    "    for i in output_path_list:\n",
    "        temp_output_file_dir = i + \"\\\\\" + 'GSEAresults' + \"\\\\\" + GSEAresultsfiletitle #name of directory to store output data\n",
    "        if count == 1:\n",
    "            GSEA_results1 = pd.read_csv(temp_output_file_dir, low_memory = False, index_col = \"Term\", header = 0, sep = ',')\n",
    "            GSEA_results1.drop(['positivetail1', 'negativetail1'], inplace=True)\n",
    "            count = count + 1\n",
    "        else:\n",
    "            GSEA_results2 = pd.read_csv(temp_output_file_dir, low_memory = False, index_col = \"Term\", header = 0, sep = ',')        \n",
    "            GSEA_results2.drop(['positivetail2', 'negativetail2'], inplace=True)\n",
    "    print('Done.')\n",
    "else: #genepanelbool = False\n",
    "    print('This section has been skipped.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db632f3f",
   "metadata": {},
   "source": [
    "#### Panel definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4976aa15",
   "metadata": {},
   "outputs": [],
   "source": [
    "if genepanelbool == True:\n",
    "    #check FDR for statistical significance\n",
    "    FDRcol1 = GSEA_results1.iloc[:, 5]\n",
    "\n",
    "    for i in range(len(FDRcol1)):\n",
    "        if FDRcol1[i] > 0.05:\n",
    "            print('FDR is too high!')\n",
    "        else:\n",
    "            print('FDR of', FDRcol1[i], 'is good for the first identification signature.')\n",
    "    print()\n",
    "\n",
    "    LEgenes1 = GSEA_results1.iloc[:, 8]\n",
    "    \n",
    "    neg_tail2 = LEgenes1.drop(['positivetail2'])\n",
    "    pos_tail2 = LEgenes1.drop(['negativetail2'])\n",
    "\n",
    "    neg_tail2 = neg_tail2.str.split(pat = \";\")  \n",
    "    pos_tail2 = pos_tail2.str.split(pat = \";\")  \n",
    "    \n",
    "    #repeat for second set of GSEA results\n",
    "    FDRcol2 = GSEA_results2.iloc[:, 5]\n",
    "\n",
    "    for i in range(len(FDRcol2)):\n",
    "        if FDRcol2[i] > 0.05:\n",
    "            print('FDR is too high!')\n",
    "        else:\n",
    "            print('FDR of', FDRcol2[i], 'is good for the first identification signature.')\n",
    "    print()\n",
    "\n",
    "    LEgenes2 = GSEA_results2.iloc[:, 8]   \n",
    "    \n",
    "    neg_tail1 = LEgenes2.drop(['positivetail1'])\n",
    "    pos_tail1 = LEgenes2.drop(['negativetail1'])\n",
    "\n",
    "    neg_tail1 = neg_tail1.str.split(pat = \";\") \n",
    "    pos_tail1 = pos_tail1.str.split(pat = \";\") \n",
    "   \n",
    "    temp_neg_panel = list(set(neg_tail1[0]) & set(neg_tail2[0]))\n",
    "    temp_pos_panel = list(set(pos_tail1[0]) & set(pos_tail2[0]))\n",
    "    \n",
    "    \n",
    "    ### ADD TO PANEL LIST HERE    \n",
    "    pos_panel_list = []\n",
    "    neg_panel_list = []\n",
    "    combo_pos_panel_list = []\n",
    "    combo_neg_panel_list = []    \n",
    "\n",
    "    pos_panel_list.append(temp_pos_panel)    \n",
    "    neg_panel_list.append(temp_neg_panel)  \n",
    "    \n",
    "    temp_neg_panel.insert(0, \"negative panel\")\n",
    "    temp_neg_panel.insert(1, \"spacer\")\n",
    "    temp_pos_panel.insert(0, \"positive panel\")\n",
    "    temp_pos_panel.insert(1, \"spacer\")\n",
    "\n",
    "    neg_panel = pd.Series(temp_neg_panel)\n",
    "    pos_panel = pd.Series(temp_pos_panel)\n",
    "\n",
    "    panels_file = pd.concat([pos_panel, neg_panel], axis=1)\n",
    "    panels_file = panels_file.T\n",
    "    \n",
    "    tempinputloc = output_gene_file_dir + '\\genepanels.gmt'\n",
    "    panels_file.to_csv(tempinputloc, sep='\\t', index=False, header = None) #name of file that stores panels in .gmt format\n",
    "    \n",
    "    print('Done.')\n",
    "else: #genepanelbool = False\n",
    "    print('This section has been skipped.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1860a8d2",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Gene meta-analysis and random model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfc4d99c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_prep_verify(input_path, output_path, dataset):\n",
    "    global cwd, dataset_list, identification_datasets\n",
    "    \n",
    "    experimental_cols3 = [] #column locations of experimental sample data\n",
    "    control_cols3 = [] #column locations of control sample data              \n",
    "    \n",
    "    #load sample identification for dataset as a list (sample_list)\n",
    "    sample_compare_path = input_path + \"\\\\\" + dataset + \"samples.txt\"\n",
    "    sample_file = open(sample_compare_path, \"r\")\n",
    "    samples = sample_file.read()\n",
    "    sample_list = samples.split(\"\\t\")\n",
    "    sample_list[-1] = sample_list[-1].strip()        \n",
    "\n",
    "    num = 0 #sample counter\n",
    "    for i in sample_list:\n",
    "        if i == '0':\n",
    "            control_cols3.append(num)\n",
    "        else:\n",
    "            experimental_cols3.append(num)\n",
    "        num = num + 1\n",
    "        \n",
    "    #load dataset as dataframe\n",
    "    probe_compare_path = input_path + \"\\\\\" + dataset + \"data.csv\"\n",
    "    df3 = pd.read_csv(probe_compare_path, low_memory = False, index_col = None, header = None, sep = ',')\n",
    "\n",
    "    df3.set_index(df3.columns[0], inplace=True)\n",
    "    df3_sample_list = df3.iloc[0].tolist()\n",
    "    df3.columns = df3.iloc[0]\n",
    "    df3.drop(df3.index[0], inplace = True)\n",
    "\n",
    "    #check number of samples between two provided files\n",
    "    if len(sample_list) == df3.shape[1]:\n",
    "        print('Total number of samples confirmed for', dataset)\n",
    "        print()\n",
    "\n",
    "    #create output directory if one does not exist\n",
    "    output_path3 = output_path + '\\GSEACompare' + dataset\n",
    "    if not os.path.exists(output_path3):\n",
    "        os.mkdir(output_path3)\n",
    "\n",
    "    #confirming experimental and control samples\n",
    "    print('Number of control samples for', dataset, ':', len(control_cols3))\n",
    "    for l in control_cols3:\n",
    "        print(df3.columns[l])\n",
    "    print()\n",
    "\n",
    "    print('Number of experimental samples for', dataset, ':', len(experimental_cols3))\n",
    "    for l in experimental_cols3:\n",
    "        print(df3.columns[l])\n",
    "    print()\n",
    "        \n",
    "    return df3, output_path3, control_cols3, experimental_cols3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e917383b",
   "metadata": {},
   "outputs": [],
   "source": [
    "if moregenedatasetsbool == True: #run verification functions from functions script \n",
    "    datasetnum = 3 #counte for loop for when more than 1 verification dataset\n",
    "    other_datasets_output_path_list = []\n",
    "        \n",
    "    #remove identification dataset from dataset_list\n",
    "    for dataset in identification_datasets:\n",
    "        dataset_list.remove(dataset)\n",
    "        \n",
    "    for dataset in dataset_list: #iterate over comparison datasets\n",
    "            \n",
    "        df3, output_path3, control_cols3, experimental_cols3 = data_prep_verify(input_file_dir, output_gene_file_dir, dataset)   \n",
    "        other_datasets_output_path_list.append(output_path3)\n",
    "        \n",
    "        normalized_df3 = zscore(df3, datasetnum, output_path3) #calls function to zscore data \n",
    "        temp_ref_sig_df3 = Tscore(normalized_df3, control_cols3, experimental_cols3, datasetnum, output_path3) #calls function to Tscore\n",
    "        ref_sig_df3 = dups(temp_ref_sig_df3, datasetnum, output_path3) #calls function to clean Tscore signature\n",
    "\n",
    "        inputdir = output_gene_file_dir + '\\genepanels.gmt'\n",
    "        prerank_GSEA(ref_sig_df3, inputdir, output_path3, gene_query_set_size) #calls function to perform GSEA\n",
    "        \n",
    "        datasetnum = datasetnum + 1\n",
    "        \n",
    "else: #moregenedatasetsbool = False\n",
    "    print('This section has been skipped.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3187e23c",
   "metadata": {},
   "source": [
    "#### Random modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bf23d39",
   "metadata": {},
   "outputs": [],
   "source": [
    "def shuffle(verifysigdf):\n",
    "    \n",
    "    #determine average size of defined panels\n",
    "    inputdir = output_gene_file_dir + '\\genepanels.gmt'        \n",
    "    dfpanel = pd.read_csv(inputdir, low_memory=False, delimiter = \"\\t\", header=None)\n",
    "\n",
    "    tempsum = 0\n",
    "    for row in dfpanel.iterrows():\n",
    "        listrow = list(row[1])\n",
    "        newlist = [x for x in listrow if pd.isnull(x) == False and x != 'nan']   \n",
    "        panellen = (len(newlist))-2\n",
    "        tempsum = tempsum + panellen\n",
    "    averagesize = tempsum/2\n",
    "    \n",
    "    #generate random gene sets of the same size as the average size of defined panels\n",
    "    df3 = pd.DataFrame()\n",
    "    \n",
    "    verifysigdf = verifysigdf.set_index('Gene')\n",
    "    listofgenes = verifysigdf.index\n",
    "    listofgenes = list(verifysigdf.index)\n",
    "    querysize = int(averagesize)\n",
    "    for i in range(1000):\n",
    "        random.shuffle(listofgenes)\n",
    "        temprandomlist = listofgenes[0:querysize]\n",
    "        temprandomlist.insert(0, ('random' + str(i)))\n",
    "        temprandomlist.insert(1, 'spacer')\n",
    "        a_series = pd.Series(temprandomlist)\n",
    "        df3 = pd.concat([df3, a_series], axis=1)\n",
    "        if i % 100 == 0:\n",
    "            print('Random list #' + str(i))\n",
    "    df3 = df3.T\n",
    "    return df3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "565a3709",
   "metadata": {},
   "outputs": [],
   "source": [
    "if randomgenemodel == True:\n",
    "    for output_path in other_datasets_output_path_list: #iterate over comparison datasets\n",
    "        #bring in other dataset signature\n",
    "        verify_sig_file = output_path + '\\signature.txt'\n",
    "        verify_sig_df = pd.read_csv(verify_sig_file, low_memory = False, index_col = None, header = 0, sep = '\\t')\n",
    "        verify_sig_df.drop(['Tscore'], axis = 1, inplace = True)\n",
    "        \n",
    "        #generate random panels\n",
    "        randompanel = shuffle(verify_sig_df)\n",
    "        randompanel.to_csv((output_gene_file_dir + '\\\\' + 'randompaneldata.gmt'), sep='\\t', index=False, header=False)\n",
    "\n",
    "        #create output directory if one does not exist\n",
    "        random_output_path = output_path + \"\\\\\" + 'random'\n",
    "        if not os.path.exists(random_output_path):\n",
    "            os.mkdir(random_output_path)\n",
    "\n",
    "        #run GSEA to generate null distibution of NES\n",
    "        dfrandomNES = prerank_GSEA(verify_sig_df, (output_gene_file_dir + '\\\\' + 'randompaneldata.gmt'), random_output_path, gene_query_set_size) \n",
    "                \n",
    "        #visualize the results\n",
    "        #first get random model NES\n",
    "        dfrandomNES = pd.read_csv((random_output_path + \"\\\\\" + 'GSEAresults' + \"\\\\\" + GSEAresultsfiletitle), low_memory=False)\n",
    "        dfrandomNEScol = dfrandomNES[\"NES\"]\n",
    "        \n",
    "        #next get panel achieved NES\n",
    "        dfNES = pd.read_csv((output_path + \"\\\\\" + 'GSEAresults' + \"\\\\\" + GSEAresultsfiletitle), low_memory=False)\n",
    "        dfNEScol = dfNES[\"NES\"]\n",
    "        \n",
    "        pospanel = dfNEScol[1]\n",
    "        if pospanel < 0:\n",
    "            negpanel = pospanel\n",
    "            pospanel = dfNEScol[0]\n",
    "\n",
    "        else:\n",
    "            negpanel = dfNEScol[0]\n",
    "\n",
    "        #box and whiskars the results\n",
    "        fig, ax = plt.subplots()\n",
    "        line1 = ax.boxplot(dfrandomNEScol, vert=0, patch_artist=True, meanline=True, showmeans=True)\n",
    "        line2 = ax.scatter(pospanel, 1, marker=\"D\", color = \"red\", label = \"positive panel\")\n",
    "        line3 = ax.scatter(negpanel, 1, marker=\"D\", color = \"blue\", label = \"negative panel\")\n",
    "        plt.yticks([1], [''])\n",
    "        plt.xlabel('Randomly Generated Normalized Enrichment Scores')\n",
    "        ax.legend(handles=[line2, line3])\n",
    "        saveloc = output_path + '\\\\randomNESboxwhisplot.png'\n",
    "        plt.savefig(saveloc, dpi=100)\n",
    "        plt.show()\n",
    "\n",
    "else: #randommodel = False\n",
    "    print('This section has been skipped.')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
